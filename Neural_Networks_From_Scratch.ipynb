{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING A NEURAL NETWORK FROM SCRATCH\n",
    "\n",
    "This notebook goes through in detail how to implement a modular neural network architecture with definable lengths and number of hidden layers and different activation functions. The initial aim was to use many different non-linear optimisation methods with this neural network and to include bias terms but time was limiting. \n",
    "\n",
    "The code is based off the code provided in this article, https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6, but is expanded upon to allow variation in the neural network architecture.\n",
    "\n",
    "The requirements to run this code is numpy, matplotlib and tqdm (any versions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a neural network\n",
    "\n",
    "A neural network, also known as artificial neural networks, are a common type of machine learning architecture that took inspriation from the brain's network of neurons. \n",
    "\n",
    "They are made of many layers of nodes, including an input layer, with each node representing a feature; one or multiple hidden layers and and output layer, which represents the prediction, as shown in the image below:\n",
    "    <img src=\"images/deep_neural_network.png\" alt=\"drawing\" width=\"500\" align='centre'/>\n",
    "<div style=\"text-align:center\">\n",
    "Fig.1 - Deep Neural Network, taken from https://www.ibm.com/uk-en/cloud/learn/neural-networks\n",
    "</div>\n",
    "\n",
    "Each node in a layer is connected to every other node in the next layer through a weight which is fine-tuned throughout training the neural network, normally using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Neural Network class\n",
    "\n",
    "The inputs for the class are:\n",
    "1. x = a 2D array for the input features for all examples used to train the neural network\n",
    "2. y = a 1D or 2D array of the true values for all examples, they are used to compare the prediction and so calculate a loss\n",
    "3. length_layers = how many nodes are in each hidden layer\n",
    "4. no_of_layers = how many hidden layers in the neural network\n",
    "5. activate_function = which activation function to use at each node\n",
    "6. a_f_derivative = the derivative the corresponding activation function\n",
    "\n",
    "```\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, \n",
    "                x,\n",
    "                y,\n",
    "                length_layers,\n",
    "                no_of_layers,\n",
    "                activate_function=sigmoid,\n",
    "                a_f_derivative= sigmoid_derivative\n",
    "                ):\n",
    "    \n",
    "```\n",
    "The weights and layers are created as a list of arrays, with the first layer being the input 'x'. This allows them to be iterated through during feedforward and backpropogation steps. The inputs are also defined as attributes of the class.\n",
    "\n",
    "```\n",
    "        self.input = x\n",
    "        self.no_of_layers = no_of_layers + 1\n",
    "        self.weights = [0] * self.no_of_layers\n",
    "        self.layers = [0] * self.no_of_layers\n",
    "        self.layers[0] = self.input\n",
    "        self.length_layers = length_layers\n",
    "        self.activate_function = activate_function\n",
    "        self.a_f_derivative = a_f_derivative\n",
    "        self.y = y\n",
    "```\n",
    "A new list, 'loss_tracker' is initalised and will be used to track the loss of the neural network for each epoch.\n",
    "```\n",
    "        self.loss_tracker = []\n",
    "```\n",
    "The weights and layers lists are iterated through and initialised so that the layer and weights arrays have the correct dimensions for the feedforward and backpropogation step.\n",
    "- The weights are given random values and are given the dimensions of (the length of the row of the previous layer, the predetermined legnth of the hidden layer)\n",
    "- The hidden layers are calculated using the activation function and numpy.dot() to generate initial values and the correct dimensions\n",
    "- The final weights is given the dimensions neccesary to produce the same number of nodes in the output layer as there are dimensions in the true value, 'y' (typically will be 1).\n",
    "```\n",
    "        for i in range(1, self.no_of_layers):\n",
    "            self.weights[i-1] = np.random.rand(self.layers[i-1].shape[1], self.length_layers)\n",
    "            self.layers[i] = self.activate_function(np.dot(self.layers[i-1], self.weights[i-1]))\n",
    "        self.weights[-1] = np.random.randn(self.length_layers, y.shape[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "\n",
    "The neural networks passes the input features through the hidden layers with the information travelling 'forward' through the network. The consecutive linear algebra on each hidden layer results in an ouput layer that is a prediction for the true values.\n",
    "\n",
    "<div style=\"text-align:centre\">\n",
    "$\\hat{y}=\\sigma(W_{1}x) + b_{1}$\n",
    "</div>\n",
    "\n",
    "For example a neural network with a single hidden layer with a single node (that does not use biases) will be simply calculating:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "$\\hat{y}=\\sigma(W_{2}\\sigma(W_{1}x))$\n",
    "</div>\n",
    "\n",
    "where $\\sigma$ represents an activation function whose function and maths will be explained below.\n",
    "\n",
    "A loss function is calculated from the difference between the prediction ($\\hat{y}$) and true value ($y$) square summed across all examples ($N$) passed through the neural network:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "$\\sum_{i=1}^{N} (y-\\hat{y})^{2}$\n",
    "</div>\n",
    "\n",
    "This loss function provides a single value for how well the neural network is predicting a value for the examples and is minimsed during backpropogation to increase the accuracy of the model.\n",
    "\n",
    "The NeuralNetwork class has a function, following the equation above, to produce a prediction so that a loss function can be calculated. It iterates through each hidden layer and finds the dot product of the layers multplied by the weights. The output layer is also calculated in the same way.\n",
    "\n",
    "```\n",
    "    def feedforward(self):\n",
    "        for i in range(1, self.no_of_layers):\n",
    "            self.layers[i] = \n",
    "                self.activate_function(np.dot(self.layers[i-1], self.weights[i-1]))\n",
    "        self.output = self.activate_function(np.dot(self.layers[i], self.weights[i]))\n",
    "     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Activation functions are used in neural networks to convert the previous layers value multplied by the weight into a form that can be passed onto the next layer. This ensures that the values of the output are restricted in their magnitude and they also add non-linearity into the neural network.\n",
    "\n",
    "For example, the sigmoid function, used in this implementation as a default, converts all values into values between 1 and 0. Therefore very large values are treated as very similiar to each other.\n",
    "\n",
    "<img src=\"images/sigmoid_function.png\" alt=\"drawing\" width=\"300\" align='centre'/>\n",
    "<div style=\"text-align:centre\">\n",
    "Fig.2 - A graph of the sigmoid function, taken from https://en.wikipedia.org/wiki/Sigmoid_function\n",
    "</div>\n",
    "\n",
    "This can be coded up using the equation:\n",
    "<div style=\"text-align:centre\">\n",
    "$S(x) = \\frac{1}{1+\\exp(-x)}$\n",
    "</div>\n",
    "\n",
    "to \n",
    "```\n",
    "    def sigmoid(x):\n",
    "        return 1.0/(1 + np.exp(-x))\n",
    "```\n",
    "\n",
    "Other activation functions are commonly used in neural networks like the ReLu, which converts all negative values to 0, and TanH function, like sigmoid but converts values into values between 1 and -1. However in this example, the sigmoid function is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropogation\n",
    "Now that a loss function has been calculated it can be used to change the weights so that this loss function is minimised. The method we are using in this neural network is gradient descent, where the change in loss with relation to the weights for each layer is used to adjust weights in order to produce better predictions during the next iteration of ```feedfoward()```. The theory is that if the slope is followed downwards you will be able to find the global minimum of the high-dimensionality space for the weights, relative to the loss. The model with the lowest loss is the best possible model, although in practise will most likely overfit to the data and perform poorly on data it was not exposed to during training.\n",
    "\n",
    "The change in loss with relation to each set of weights is done by working backwards through the network, hence 'back' with the gradients of the loss of function relative to the weights used to calculate the preceding weight's gradient, hence 'propogate'. \n",
    "\n",
    "Mathematically this is achieved using the chain rule, with the following example displaying this methodology:\n",
    "\n",
    "STEP 1. Consider a simple neural network with two hidden layers, $a^{L}$ and $a^{L-1}$ (pictured) where:  \n",
    "<div style=\"text-align:center\">\n",
    "<div>\n",
    "$z^{L}= a^{L-1}w^{L-1}+b^{L-1}$\n",
    "</div>\n",
    "<div>\n",
    "$a^{L} = \\sigma(z^{L})$\n",
    "</div>\n",
    "<div>\n",
    "$C = (a^{L} - y)^{2}$\n",
    "</div>\n",
    "<img src=\"images/backpropogation_example.png\" alt=\"drawing\" width=\"400\" align='centre'/>\n",
    "<div style=\"text-align:center\">\n",
    "Fig.3 A simplified representation of the final layer of a neural network, taken from https://towardsdatascience.com/the-maths-behind-back-propagation-cf6714736abf\n",
    "</div>\n",
    "\n",
    "STEP 2. In order to get $\\frac{\\delta C}{\\delta W^{L-1}}$, and so do gradient descent, we need to calculate  $\\frac{\\delta C}{\\delta a^{L}}$,  $\\frac{\\delta a^{L}}{\\delta z^{L}}$, $\\frac{\\delta z^{L}}{\\delta w^{L-1}}$ and multiply them together\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<div> $\\frac{\\delta C}{\\delta a^{L}} = 2(a^{L}- y)$ as $C = (a^{L}- y)^{2}$ </div>\n",
    "<div> $\\frac{\\delta a^{L}}{\\delta z^{L}} = \\sigma'(z^{L})$ where  $\\sigma'(z^{L})$ is the derivative sigmoid function of $z^{L}$, which is included as a input of the class</div>\n",
    "<div>$\\frac{\\delta z^{L}}{\\delta w^{L-1}} = a^{L-1} $ as $z^{L}= a^{L-1}w^{L-1}+b^{L-1}$ </div>\n",
    "</div>\n",
    "\n",
    "STEP 3. Therefore to calculate the $\\frac{\\delta C}{\\delta W^{L-2}}$ for the next set of weights requires us to continue to follow along this chain so $\\frac{\\delta C}{\\delta W^{L-1}}$ can be broken down into the product of $\\frac{\\delta C}{\\delta a^{L-1}}$,  $\\frac{\\delta a^{L-1}}{\\delta z^{L-1}}$, $\\frac{\\delta z^{L-1}}{\\delta w^{L-2}}$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<div> $\\frac{\\delta a^{L-1}}{\\delta z^{L-1}} = \\sigma'(z^{L-1})$ where  $\\sigma'(z^{L-1})$ is the derivative sigmoid function of $z^{L-1}$ </div>\n",
    "<div>$\\frac{\\delta z^{L-1}}{\\delta w^{L-2}} = a^{L-2} $ as $z^{L-1}= a^{L-2}w^{L-2}+b^{L-2}$ </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "However calculating $\\frac{\\delta C}{\\delta a^{L-1}}$ requries further decomposition into separate derivatives, $\\frac{\\delta C}{\\delta a^{L}}$,  $\\frac{\\delta a^{L}}{\\delta z^{L}}$ and $\\frac{\\delta z^{L}}{\\delta a^{L-1}}$ \n",
    "<div style=\"text-align:center\">\n",
    "$\\frac{\\delta z^{L}}{\\delta a^{L-1}} = w^{L-1}$ as $z^{L}= a^{L-1}w^{L-1}+b^{L-1}$\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "\n",
    "$\\frac{\\delta C}{\\delta a^{L}}$ and $\\frac{\\delta a^{L}}{\\delta z^{L}}$ have already been calculated in STEP 2 so we can just use calculated values. If you continue going back through the weights, you will find that each derivative of C with relation to the weights contains the product of derivatives calculated for later weights.\n",
    "\n",
    "This is taken advantage of in the ```backpropogate()``` function which uses a temporary variable ```propogate_weights``` to keep track of derivatives needed for future calculations as it iterates through the weights. This function is quite complicated but is concisely doing the calculus described above.\n",
    "\n",
    "```\n",
    "    def backpropropgate(self):\n",
    "        for i in range(self.no_of_layers-1, -1, -1):\n",
    "            if i == self.no_of_layers-1:\n",
    "                weight_propogate = 2*(self.y-self.output) * self.a_f_derivative(self.output)\n",
    "                self.weights[i] += np.dot(self.layers[i].T, weight_propogate)  \n",
    "            else:\n",
    "                self.weights[i] += np.dot(self.layers[i].T, (np.dot(weight_propogate, self.weights[i+1].T)* self.a_f_derivative(self.layers[i+1])))\n",
    "                weight_propogate = (np.dot(weight_propogate, self.weights[i+1].T)* self.a_f_derivative(self.layers[i+1]))\n",
    "        self.track_loss(sum((self.y - self.output)**2))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other functions\n",
    "\n",
    "In order to train the neural network, we must repeatedly feedforward to produce a new cost and then use that cost function to adjust the weights. THis is done using the function ```train()``` which takes in the number of epochs, the number of times the network is retrained on the data, as input. \n",
    "\n",
    "\n",
    "```\n",
    "    def train(self, iterations):\n",
    "        for i in tqdm(range(iterations)):\n",
    "            self.feedforward()\n",
    "            self.backpropropgate()\n",
    "        print(self.output)\n",
    "        plt.scatter(list(range(0, iterations)), self.loss_tracker)\n",
    "        plt.yscale('log')\n",
    "        plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "In order to see how quickly the loss is reduced for each epoch, we will save the loss to a list using ```track_loss()``` \n",
    "```\n",
    "    def track_loss(self, loss):\n",
    "        self.loss_tracker.append(loss)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A test\n",
    "To test the new this neural network, we are training on the small dataset provided in https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6. In theory it can be used on any dataset but I have not managed to find parameters that work on larger datasets and training time takes too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import NeuralNetwork\n",
    "from activation_functions import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 12537.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted values are [[0.11131968]\n",
      " [0.91408829]\n",
      " [0.91408839]\n",
      " [0.10648309]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Features\n",
    "X = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]])\n",
    "# True values\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# An instance of a neural network with one hidden layer, with length 2, using the sigmoid function as the activation function\n",
    "nn = NeuralNetwork(X,y, 2, 1, activate_function=sigmoid, a_f_derivative=sigmoid_derivative)\n",
    "\n",
    "nn.train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
